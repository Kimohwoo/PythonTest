{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "x_data = torch.FloatTensor([[1],[2],[3],[4],[5]])\n",
    "t_data = torch.FloatTensor([[3],[5],[7],[9],[11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[0.5153]], requires_grad=True), Parameter containing:\n",
      "tensor([-0.4414], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(1,1) # 입력 크기, 출력 크기 -> input_dim = 1, output_dim = 1\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr = 0.01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 39.16685104370117\n",
      "Epoch: 100 Cost: 0.08463062345981598\n",
      "Epoch: 200 Cost: 0.04298952594399452\n",
      "Epoch: 300 Cost: 0.021837208420038223\n",
      "Epoch: 400 Cost: 0.01109258271753788\n",
      "Epoch: 500 Cost: 0.00563468411564827\n",
      "Epoch: 600 Cost: 0.0028622171375900507\n",
      "Epoch: 700 Cost: 0.0014539161929860711\n",
      "Epoch: 800 Cost: 0.0007385412463918328\n",
      "Epoch: 900 Cost: 0.00037515218718908727\n",
      "Epoch: 1000 Cost: 0.00019056595920119435\n",
      "Epoch: 1100 Cost: 9.680409129941836e-05\n",
      "Epoch: 1200 Cost: 4.91717473778408e-05\n",
      "Epoch: 1300 Cost: 2.497919740562793e-05\n",
      "Epoch: 1400 Cost: 1.2689983122982085e-05\n",
      "Epoch: 1500 Cost: 6.447205578297144e-06\n",
      "Epoch: 1600 Cost: 3.276181814726442e-06\n",
      "Epoch: 1700 Cost: 1.6654261116855196e-06\n",
      "Epoch: 1800 Cost: 8.460160643153358e-07\n",
      "Epoch: 1900 Cost: 4.302897309571563e-07\n",
      "Epoch: 2000 Cost: 2.1860606125301274e-07\n",
      "Epoch: 2100 Cost: 1.1113379372318377e-07\n",
      "Epoch: 2200 Cost: 5.657594925878584e-08\n",
      "Epoch: 2300 Cost: 2.86554158179797e-08\n",
      "Epoch: 2400 Cost: 1.4702209227834828e-08\n",
      "Epoch: 2500 Cost: 7.451364858468423e-09\n",
      "Epoch: 2600 Cost: 3.771356116288871e-09\n",
      "Epoch: 2700 Cost: 1.905709812177747e-09\n",
      "Epoch: 2800 Cost: 9.986707016906848e-10\n",
      "Epoch: 2900 Cost: 5.25824384034479e-10\n",
      "Epoch: 3000 Cost: 2.9467628337442875e-10\n",
      "Epoch: 3100 Cost: 1.63220187365809e-10\n",
      "Epoch: 3200 Cost: 1.0809344391793374e-10\n",
      "Epoch: 3300 Cost: 6.230038707144558e-11\n",
      "Epoch: 3400 Cost: 5.426272625674855e-11\n",
      "Epoch: 3500 Cost: 5.426272625674855e-11\n",
      "Epoch: 3600 Cost: 5.426272625674855e-11\n",
      "Epoch: 3700 Cost: 5.426272625674855e-11\n",
      "Epoch: 3800 Cost: 5.426272625674855e-11\n",
      "Epoch: 3900 Cost: 5.426272625674855e-11\n",
      "Epoch: 4000 Cost: 5.426272625674855e-11\n",
      "Epoch: 4100 Cost: 5.426272625674855e-11\n",
      "Epoch: 4200 Cost: 5.426272625674855e-11\n",
      "Epoch: 4300 Cost: 5.426272625674855e-11\n",
      "Epoch: 4400 Cost: 5.426272625674855e-11\n",
      "Epoch: 4500 Cost: 5.426272625674855e-11\n",
      "Epoch: 4600 Cost: 5.426272625674855e-11\n",
      "Epoch: 4700 Cost: 5.426272625674855e-11\n",
      "Epoch: 4800 Cost: 5.426272625674855e-11\n",
      "Epoch: 4900 Cost: 5.426272625674855e-11\n",
      "Epoch: 5000 Cost: 5.426272625674855e-11\n",
      "Epoch: 5100 Cost: 5.426272625674855e-11\n",
      "Epoch: 5200 Cost: 5.426272625674855e-11\n",
      "Epoch: 5300 Cost: 5.426272625674855e-11\n",
      "Epoch: 5400 Cost: 5.426272625674855e-11\n",
      "Epoch: 5500 Cost: 5.426272625674855e-11\n",
      "Epoch: 5600 Cost: 5.426272625674855e-11\n",
      "Epoch: 5700 Cost: 5.426272625674855e-11\n",
      "Epoch: 5800 Cost: 5.426272625674855e-11\n",
      "Epoch: 5900 Cost: 5.426272625674855e-11\n",
      "Epoch: 6000 Cost: 5.426272625674855e-11\n",
      "Epoch: 6100 Cost: 5.426272625674855e-11\n",
      "Epoch: 6200 Cost: 5.426272625674855e-11\n",
      "Epoch: 6300 Cost: 5.426272625674855e-11\n",
      "Epoch: 6400 Cost: 5.426272625674855e-11\n",
      "Epoch: 6500 Cost: 5.426272625674855e-11\n",
      "Epoch: 6600 Cost: 5.426272625674855e-11\n",
      "Epoch: 6700 Cost: 5.426272625674855e-11\n",
      "Epoch: 6800 Cost: 5.426272625674855e-11\n",
      "Epoch: 6900 Cost: 5.426272625674855e-11\n",
      "Epoch: 7000 Cost: 5.426272625674855e-11\n",
      "Epoch: 7100 Cost: 5.426272625674855e-11\n",
      "Epoch: 7200 Cost: 5.426272625674855e-11\n",
      "Epoch: 7300 Cost: 5.426272625674855e-11\n",
      "Epoch: 7400 Cost: 5.426272625674855e-11\n",
      "Epoch: 7500 Cost: 5.426272625674855e-11\n",
      "Epoch: 7600 Cost: 5.426272625674855e-11\n",
      "Epoch: 7700 Cost: 5.426272625674855e-11\n",
      "Epoch: 7800 Cost: 5.426272625674855e-11\n",
      "Epoch: 7900 Cost: 5.426272625674855e-11\n",
      "Epoch: 8000 Cost: 5.426272625674855e-11\n",
      "Epoch: 8100 Cost: 5.426272625674855e-11\n",
      "Epoch: 8200 Cost: 5.426272625674855e-11\n",
      "Epoch: 8300 Cost: 5.426272625674855e-11\n",
      "Epoch: 8400 Cost: 5.426272625674855e-11\n",
      "Epoch: 8500 Cost: 5.426272625674855e-11\n",
      "Epoch: 8600 Cost: 5.426272625674855e-11\n",
      "Epoch: 8700 Cost: 5.426272625674855e-11\n",
      "Epoch: 8800 Cost: 5.426272625674855e-11\n",
      "Epoch: 8900 Cost: 5.426272625674855e-11\n",
      "Epoch: 9000 Cost: 5.426272625674855e-11\n",
      "Epoch: 9100 Cost: 5.426272625674855e-11\n",
      "Epoch: 9200 Cost: 5.426272625674855e-11\n",
      "Epoch: 9300 Cost: 5.426272625674855e-11\n",
      "Epoch: 9400 Cost: 5.426272625674855e-11\n",
      "Epoch: 9500 Cost: 5.426272625674855e-11\n",
      "Epoch: 9600 Cost: 5.426272625674855e-11\n",
      "Epoch: 9700 Cost: 5.426272625674855e-11\n",
      "Epoch: 9800 Cost: 5.426272625674855e-11\n",
      "Epoch: 9900 Cost: 5.426272625674855e-11\n",
      "Epoch: 10000 Cost: 5.426272625674855e-11\n",
      "Epoch: 10100 Cost: 5.426272625674855e-11\n",
      "Epoch: 10200 Cost: 5.426272625674855e-11\n",
      "Epoch: 10300 Cost: 5.426272625674855e-11\n",
      "Epoch: 10400 Cost: 5.426272625674855e-11\n",
      "Epoch: 10500 Cost: 5.426272625674855e-11\n",
      "Epoch: 10600 Cost: 5.426272625674855e-11\n",
      "Epoch: 10700 Cost: 5.426272625674855e-11\n",
      "Epoch: 10800 Cost: 5.426272625674855e-11\n",
      "Epoch: 10900 Cost: 5.426272625674855e-11\n",
      "Epoch: 11000 Cost: 5.426272625674855e-11\n",
      "Epoch: 11100 Cost: 5.426272625674855e-11\n",
      "Epoch: 11200 Cost: 5.426272625674855e-11\n",
      "Epoch: 11300 Cost: 5.426272625674855e-11\n",
      "Epoch: 11400 Cost: 5.426272625674855e-11\n",
      "Epoch: 11500 Cost: 5.426272625674855e-11\n",
      "Epoch: 11600 Cost: 5.426272625674855e-11\n",
      "Epoch: 11700 Cost: 5.426272625674855e-11\n",
      "Epoch: 11800 Cost: 5.426272625674855e-11\n",
      "Epoch: 11900 Cost: 5.426272625674855e-11\n",
      "Epoch: 12000 Cost: 5.426272625674855e-11\n",
      "Epoch: 12100 Cost: 5.426272625674855e-11\n",
      "Epoch: 12200 Cost: 5.426272625674855e-11\n",
      "Epoch: 12300 Cost: 5.426272625674855e-11\n",
      "Epoch: 12400 Cost: 5.426272625674855e-11\n",
      "Epoch: 12500 Cost: 5.426272625674855e-11\n",
      "Epoch: 12600 Cost: 5.426272625674855e-11\n",
      "Epoch: 12700 Cost: 5.426272625674855e-11\n",
      "Epoch: 12800 Cost: 5.426272625674855e-11\n",
      "Epoch: 12900 Cost: 5.426272625674855e-11\n",
      "Epoch: 13000 Cost: 5.426272625674855e-11\n",
      "Epoch: 13100 Cost: 5.426272625674855e-11\n",
      "Epoch: 13200 Cost: 5.426272625674855e-11\n",
      "Epoch: 13300 Cost: 5.426272625674855e-11\n",
      "Epoch: 13400 Cost: 5.426272625674855e-11\n",
      "Epoch: 13500 Cost: 5.426272625674855e-11\n",
      "Epoch: 13600 Cost: 5.426272625674855e-11\n",
      "Epoch: 13700 Cost: 5.426272625674855e-11\n",
      "Epoch: 13800 Cost: 5.426272625674855e-11\n",
      "Epoch: 13900 Cost: 5.426272625674855e-11\n",
      "Epoch: 14000 Cost: 5.426272625674855e-11\n",
      "Epoch: 14100 Cost: 5.426272625674855e-11\n",
      "Epoch: 14200 Cost: 5.426272625674855e-11\n",
      "Epoch: 14300 Cost: 5.426272625674855e-11\n",
      "Epoch: 14400 Cost: 5.426272625674855e-11\n",
      "Epoch: 14500 Cost: 5.426272625674855e-11\n",
      "Epoch: 14600 Cost: 5.426272625674855e-11\n",
      "Epoch: 14700 Cost: 5.426272625674855e-11\n",
      "Epoch: 14800 Cost: 5.426272625674855e-11\n",
      "Epoch: 14900 Cost: 5.426272625674855e-11\n",
      "Epoch: 15000 Cost: 5.426272625674855e-11\n",
      "Epoch: 15100 Cost: 5.426272625674855e-11\n",
      "Epoch: 15200 Cost: 5.426272625674855e-11\n",
      "Epoch: 15300 Cost: 5.426272625674855e-11\n",
      "Epoch: 15400 Cost: 5.426272625674855e-11\n",
      "Epoch: 15500 Cost: 5.426272625674855e-11\n",
      "Epoch: 15600 Cost: 5.426272625674855e-11\n",
      "Epoch: 15700 Cost: 5.426272625674855e-11\n",
      "Epoch: 15800 Cost: 5.426272625674855e-11\n",
      "Epoch: 15900 Cost: 5.426272625674855e-11\n",
      "Epoch: 16000 Cost: 5.426272625674855e-11\n",
      "Epoch: 16100 Cost: 5.426272625674855e-11\n",
      "Epoch: 16200 Cost: 5.426272625674855e-11\n",
      "Epoch: 16300 Cost: 5.426272625674855e-11\n",
      "Epoch: 16400 Cost: 5.426272625674855e-11\n",
      "Epoch: 16500 Cost: 5.426272625674855e-11\n",
      "Epoch: 16600 Cost: 5.426272625674855e-11\n",
      "Epoch: 16700 Cost: 5.426272625674855e-11\n",
      "Epoch: 16800 Cost: 5.426272625674855e-11\n",
      "Epoch: 16900 Cost: 5.426272625674855e-11\n",
      "Epoch: 17000 Cost: 5.426272625674855e-11\n",
      "Epoch: 17100 Cost: 5.426272625674855e-11\n",
      "Epoch: 17200 Cost: 5.426272625674855e-11\n",
      "Epoch: 17300 Cost: 5.426272625674855e-11\n",
      "Epoch: 17400 Cost: 5.426272625674855e-11\n",
      "Epoch: 17500 Cost: 5.426272625674855e-11\n",
      "Epoch: 17600 Cost: 5.426272625674855e-11\n",
      "Epoch: 17700 Cost: 5.426272625674855e-11\n",
      "Epoch: 17800 Cost: 5.426272625674855e-11\n",
      "Epoch: 17900 Cost: 5.426272625674855e-11\n",
      "Epoch: 18000 Cost: 5.426272625674855e-11\n",
      "Epoch: 18100 Cost: 5.426272625674855e-11\n",
      "Epoch: 18200 Cost: 5.426272625674855e-11\n",
      "Epoch: 18300 Cost: 5.426272625674855e-11\n",
      "Epoch: 18400 Cost: 5.426272625674855e-11\n",
      "Epoch: 18500 Cost: 5.426272625674855e-11\n",
      "Epoch: 18600 Cost: 5.426272625674855e-11\n",
      "Epoch: 18700 Cost: 5.426272625674855e-11\n",
      "Epoch: 18800 Cost: 5.426272625674855e-11\n",
      "Epoch: 18900 Cost: 5.426272625674855e-11\n",
      "Epoch: 19000 Cost: 5.426272625674855e-11\n",
      "Epoch: 19100 Cost: 5.426272625674855e-11\n",
      "Epoch: 19200 Cost: 5.426272625674855e-11\n",
      "Epoch: 19300 Cost: 5.426272625674855e-11\n",
      "Epoch: 19400 Cost: 5.426272625674855e-11\n",
      "Epoch: 19500 Cost: 5.426272625674855e-11\n",
      "Epoch: 19600 Cost: 5.426272625674855e-11\n",
      "Epoch: 19700 Cost: 5.426272625674855e-11\n",
      "Epoch: 19800 Cost: 5.426272625674855e-11\n",
      "Epoch: 19900 Cost: 5.426272625674855e-11\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 20000\n",
    "\n",
    "for epoch in range(nb_epochs):\n",
    "    prediction = model(x_data)\n",
    "    cost = F.mse_loss(prediction, t_data)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', epoch, 'Cost:', cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([21.0000], grad_fn=<AddBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[2.0000]], requires_grad=True), Parameter containing:\n",
      "tensor([1.0000], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "new_data = torch.FloatTensor([10])\n",
    "y_pred = model(new_data)\n",
    "print(y_pred)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_tain = torch.FloatTensor([[73., 80., 75.],\n",
    "                            [93., 88., 93.],\n",
    "                            [89., 91., 90.],\n",
    "                            [96., 98., 100.],\n",
    "                            [73., 66., 70.]])\n",
    "t_train = torch.FloatTensor([[152.],\n",
    "                            [185.],\n",
    "                            [180.],\n",
    "                            [196.],\n",
    "                            [142.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Parameter containing:\n",
      "tensor([[-0.1119,  0.2710, -0.5435]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3462], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "model = nn.Linear(3,1)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.SGD(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 0.16346625983715057\n",
      "Epoch: 400 Cost: 0.1634562909603119\n",
      "Epoch: 800 Cost: 0.16344048082828522\n",
      "Epoch: 1200 Cost: 0.16342493891716003\n",
      "Epoch: 1600 Cost: 0.16340823471546173\n",
      "Epoch: 2000 Cost: 0.16339616477489471\n",
      "Epoch: 2400 Cost: 0.16338039934635162\n",
      "Epoch: 2800 Cost: 0.16337010264396667\n",
      "Epoch: 3200 Cost: 0.1633537858724594\n",
      "Epoch: 3600 Cost: 0.1633395403623581\n",
      "Epoch: 4000 Cost: 0.16332539916038513\n",
      "Epoch: 4400 Cost: 0.16331364214420319\n",
      "Epoch: 4800 Cost: 0.1633032113313675\n",
      "Epoch: 5200 Cost: 0.16328655183315277\n",
      "Epoch: 5600 Cost: 0.16327118873596191\n",
      "Epoch: 6000 Cost: 0.1632615625858307\n",
      "Epoch: 6400 Cost: 0.16324792802333832\n",
      "Epoch: 6800 Cost: 0.16323406994342804\n",
      "Epoch: 7200 Cost: 0.16321973502635956\n",
      "Epoch: 7600 Cost: 0.16320838034152985\n",
      "Epoch: 8000 Cost: 0.16319569945335388\n",
      "Epoch: 8400 Cost: 0.16317962110042572\n",
      "Epoch: 8800 Cost: 0.1631644070148468\n",
      "Epoch: 9200 Cost: 0.1631498634815216\n",
      "Epoch: 9600 Cost: 0.16314232349395752\n",
      "Epoch: 10000 Cost: 0.16312938928604126\n",
      "Epoch: 10400 Cost: 0.1631138175725937\n",
      "Epoch: 10800 Cost: 0.16310268640518188\n",
      "Epoch: 11200 Cost: 0.16308584809303284\n",
      "Epoch: 11600 Cost: 0.1630767285823822\n",
      "Epoch: 12000 Cost: 0.16306395828723907\n",
      "Epoch: 12400 Cost: 0.1630506068468094\n",
      "Epoch: 12800 Cost: 0.16303935647010803\n",
      "Epoch: 13200 Cost: 0.1630268394947052\n",
      "Epoch: 13600 Cost: 0.16301527619361877\n",
      "Epoch: 14000 Cost: 0.16300442814826965\n",
      "Epoch: 14400 Cost: 0.16298946738243103\n",
      "Epoch: 14800 Cost: 0.16298089921474457\n",
      "Epoch: 15200 Cost: 0.16296613216400146\n",
      "Epoch: 15600 Cost: 0.16295450925827026\n",
      "Epoch: 16000 Cost: 0.1629386991262436\n",
      "Epoch: 16400 Cost: 0.16292378306388855\n",
      "Epoch: 16800 Cost: 0.16291776299476624\n",
      "Epoch: 17200 Cost: 0.16290289163589478\n",
      "Epoch: 17600 Cost: 0.16288712620735168\n",
      "Epoch: 18000 Cost: 0.1628832370042801\n",
      "Epoch: 18400 Cost: 0.16286823153495789\n",
      "Epoch: 18800 Cost: 0.1628575623035431\n",
      "Epoch: 19200 Cost: 0.16284066438674927\n",
      "Epoch: 19600 Cost: 0.1628350466489792\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    prediction = model(x_tain)\n",
    "    cost = F.mse_loss(prediction, t_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 400 == 0 :\n",
    "        print('Epoch:', epoch, 'Cost:', cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[153.7640]], grad_fn=<AddmmBackward0>)\n",
      "[Parameter containing:\n",
      "tensor([[1.0707, 0.5359, 0.4031]], requires_grad=True), Parameter containing:\n",
      "tensor([0.3584], requires_grad=True)]\n"
     ]
    }
   ],
   "source": [
    "new_train = torch.FloatTensor([[75,80,75]])\n",
    "y_pred = model(new_train)\n",
    "print(y_pred)\n",
    "print(list(model.parameters()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# class 사용 single regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "x_data = torch.FloatTensor([[1],[2],[3],[4],[5]])\n",
    "t_data = torch.FloatTensor([[3],[5],[7],[9],[11]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(1,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 39.16685104370117\n",
      "Epoch: 400 Cost: 0.01109258271753788\n",
      "Epoch: 800 Cost: 0.0007385412463918328\n",
      "Epoch: 1200 Cost: 4.91717473778408e-05\n",
      "Epoch: 1600 Cost: 3.276181814726442e-06\n",
      "Epoch: 2000 Cost: 2.1860606125301274e-07\n",
      "Epoch: 2400 Cost: 1.4702209227834828e-08\n",
      "Epoch: 2800 Cost: 9.986707016906848e-10\n",
      "Epoch: 3200 Cost: 1.0809344391793374e-10\n",
      "Epoch: 3600 Cost: 5.426272625674855e-11\n",
      "Epoch: 4000 Cost: 5.426272625674855e-11\n",
      "Epoch: 4400 Cost: 5.426272625674855e-11\n",
      "Epoch: 4800 Cost: 5.426272625674855e-11\n",
      "Epoch: 5200 Cost: 5.426272625674855e-11\n",
      "Epoch: 5600 Cost: 5.426272625674855e-11\n",
      "Epoch: 6000 Cost: 5.426272625674855e-11\n",
      "Epoch: 6400 Cost: 5.426272625674855e-11\n",
      "Epoch: 6800 Cost: 5.426272625674855e-11\n",
      "Epoch: 7200 Cost: 5.426272625674855e-11\n",
      "Epoch: 7600 Cost: 5.426272625674855e-11\n",
      "Epoch: 8000 Cost: 5.426272625674855e-11\n",
      "Epoch: 8400 Cost: 5.426272625674855e-11\n",
      "Epoch: 8800 Cost: 5.426272625674855e-11\n",
      "Epoch: 9200 Cost: 5.426272625674855e-11\n",
      "Epoch: 9600 Cost: 5.426272625674855e-11\n",
      "Epoch: 10000 Cost: 5.426272625674855e-11\n",
      "Epoch: 10400 Cost: 5.426272625674855e-11\n",
      "Epoch: 10800 Cost: 5.426272625674855e-11\n",
      "Epoch: 11200 Cost: 5.426272625674855e-11\n",
      "Epoch: 11600 Cost: 5.426272625674855e-11\n",
      "Epoch: 12000 Cost: 5.426272625674855e-11\n",
      "Epoch: 12400 Cost: 5.426272625674855e-11\n",
      "Epoch: 12800 Cost: 5.426272625674855e-11\n",
      "Epoch: 13200 Cost: 5.426272625674855e-11\n",
      "Epoch: 13600 Cost: 5.426272625674855e-11\n",
      "Epoch: 14000 Cost: 5.426272625674855e-11\n",
      "Epoch: 14400 Cost: 5.426272625674855e-11\n",
      "Epoch: 14800 Cost: 5.426272625674855e-11\n",
      "Epoch: 15200 Cost: 5.426272625674855e-11\n",
      "Epoch: 15600 Cost: 5.426272625674855e-11\n",
      "Epoch: 16000 Cost: 5.426272625674855e-11\n",
      "Epoch: 16400 Cost: 5.426272625674855e-11\n",
      "Epoch: 16800 Cost: 5.426272625674855e-11\n",
      "Epoch: 17200 Cost: 5.426272625674855e-11\n",
      "Epoch: 17600 Cost: 5.426272625674855e-11\n",
      "Epoch: 18000 Cost: 5.426272625674855e-11\n",
      "Epoch: 18400 Cost: 5.426272625674855e-11\n",
      "Epoch: 18800 Cost: 5.426272625674855e-11\n",
      "Epoch: 19200 Cost: 5.426272625674855e-11\n",
      "Epoch: 19600 Cost: 5.426272625674855e-11\n"
     ]
    }
   ],
   "source": [
    "model = LinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.01)\n",
    "\n",
    "nb_epochs = 20000\n",
    "for epoch in range(nb_epochs):\n",
    "    prediction = model(x_data)\n",
    "    cost = F.mse_loss(prediction, t_data)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 400 == 0:\n",
    "        print('Epoch:', epoch, 'Cost:', cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[4.0000]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_data = torch.FloatTensor([[1.5]])\n",
    "print(model(new_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = torch.FloatTensor([[73., 80., 75.],\n",
    "                            [93., 88., 93.],\n",
    "                            [89., 91., 90.],\n",
    "                            [96., 98., 100.],\n",
    "                            [73., 66., 70.]])\n",
    "t_train = torch.FloatTensor([[152.],\n",
    "                            [185.],\n",
    "                            [180.],\n",
    "                            [196.],\n",
    "                            [142.]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegressionModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLinearRegressionModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# nb_epochs = 20000\n",
    "for epoch in range(nb_epochs):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, t_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 400 == 0:\n",
    "        print('Epoch:', epoch, 'Cost:', cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[151.2944]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_data = torch.FloatTensor([[73., 80., 75.]])\n",
    "pred_y = model(new_data)\n",
    "print(pred_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 73.  80.  75.]\n",
      " [ 93.  88.  93.]\n",
      " [ 89.  91.  90.]\n",
      " [ 96.  98. 100.]\n",
      " [ 73.  66.  70.]]\n",
      "[[152.]\n",
      " [185.]\n",
      " [180.]\n",
      " [196.]\n",
      " [142.]]\n",
      "tensor([[ 73.,  80.,  75.],\n",
      "        [ 93.,  88.,  93.],\n",
      "        [ 89.,  91.,  90.],\n",
      "        [ 96.,  98., 100.],\n",
      "        [ 73.,  66.,  70.]])\n",
      "tensor([[152.],\n",
      "        [185.],\n",
      "        [180.],\n",
      "        [196.],\n",
      "        [142.]])\n"
     ]
    }
   ],
   "source": [
    "data = np.loadtxt('data/data-01-test-score.csv', delimiter=',', dtype='float32')\n",
    "x_data = data[:, 0:-1]\n",
    "t_data = data[:, [-1]]\n",
    "print(x_data[:5])\n",
    "print(t_data[:5])\n",
    "x_train = torch.from_numpy(x_data)\n",
    "y_train = torch.from_numpy(t_data)\n",
    "print(x_tain[:5])\n",
    "print(y_train[:5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(1)\n",
    "\n",
    "model = nn.Linear(3,1)\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 199,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: 28693.490234375\n",
      "Epoch: 4000 Cost: 13.772063255310059\n",
      "Epoch: 8000 Cost: 12.176605224609375\n",
      "Epoch: 12000 Cost: 10.934386253356934\n",
      "Epoch: 16000 Cost: 9.962173461914062\n",
      "Epoch: 20000 Cost: 9.197564125061035\n",
      "Epoch: 24000 Cost: 8.593523979187012\n",
      "Epoch: 28000 Cost: 8.114365577697754\n",
      "Epoch: 32000 Cost: 7.732822418212891\n",
      "Epoch: 36000 Cost: 7.42799186706543\n",
      "Epoch: 40000 Cost: 7.183600425720215\n",
      "Epoch: 44000 Cost: 6.987161159515381\n",
      "Epoch: 48000 Cost: 6.828953266143799\n",
      "Epoch: 52000 Cost: 6.701241970062256\n",
      "Epoch: 56000 Cost: 6.597883701324463\n",
      "Epoch: 60000 Cost: 6.514108657836914\n",
      "Epoch: 64000 Cost: 6.446160316467285\n",
      "Epoch: 68000 Cost: 6.390915393829346\n",
      "Epoch: 72000 Cost: 6.3459367752075195\n",
      "Epoch: 76000 Cost: 6.309393405914307\n",
      "Epoch: 80000 Cost: 6.279495716094971\n",
      "Epoch: 84000 Cost: 6.255125045776367\n",
      "Epoch: 88000 Cost: 6.23521614074707\n",
      "Epoch: 92000 Cost: 6.218939781188965\n",
      "Epoch: 96000 Cost: 6.205629348754883\n"
     ]
    }
   ],
   "source": [
    "nb_epochs = 100000\n",
    "for epoch in range(nb_epochs):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 4000 == 0:\n",
    "        print('Epoch:', epoch, 'Cost:', cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 213,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[152.8082]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "new_data = torch.FloatTensor([[73,80,75]])\n",
    "pred = model(new_data)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 210,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiLinearRegressModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(3,1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 211,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MultiLinearRegressModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=1e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 212,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: tensor(28388., grad_fn=<MseLossBackward0>)\n",
      "Epoch: 400 Cost: tensor(13.2477, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 800 Cost: tensor(13.0749, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 1200 Cost: tensor(12.9066, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 1600 Cost: tensor(12.7427, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 2000 Cost: tensor(12.5831, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 2400 Cost: tensor(12.4276, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 2800 Cost: tensor(12.2761, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 3200 Cost: tensor(12.1286, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 3600 Cost: tensor(11.9848, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 4000 Cost: tensor(11.8447, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 4400 Cost: tensor(11.7083, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 4800 Cost: tensor(11.5753, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 5200 Cost: tensor(11.4457, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 5600 Cost: tensor(11.3194, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 6000 Cost: tensor(11.1963, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 6400 Cost: tensor(11.0764, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 6800 Cost: tensor(10.9595, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 7200 Cost: tensor(10.8455, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 7600 Cost: tensor(10.7344, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 8000 Cost: tensor(10.6261, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 8400 Cost: tensor(10.5206, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 8800 Cost: tensor(10.4176, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 9200 Cost: tensor(10.3173, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 9600 Cost: tensor(10.2194, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 10000 Cost: tensor(10.1240, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 10400 Cost: tensor(10.0310, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 10800 Cost: tensor(9.9403, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 11200 Cost: tensor(9.8517, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 11600 Cost: tensor(9.7654, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 12000 Cost: tensor(9.6813, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 12400 Cost: tensor(9.5991, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 12800 Cost: tensor(9.5190, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 13200 Cost: tensor(9.4409, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 13600 Cost: tensor(9.3646, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 14000 Cost: tensor(9.2902, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 14400 Cost: tensor(9.2177, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 14800 Cost: tensor(9.1468, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 15200 Cost: tensor(9.0777, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 15600 Cost: tensor(9.0103, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 16000 Cost: tensor(8.9445, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 16400 Cost: tensor(8.8803, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 16800 Cost: tensor(8.8176, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 17200 Cost: tensor(8.7565, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 17600 Cost: tensor(8.6968, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 18000 Cost: tensor(8.6385, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 18400 Cost: tensor(8.5816, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 18800 Cost: tensor(8.5260, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 19200 Cost: tensor(8.4718, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 19600 Cost: tensor(8.4189, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 20000 Cost: tensor(8.3672, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 20400 Cost: tensor(8.3167, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 20800 Cost: tensor(8.2675, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 21200 Cost: tensor(8.2193, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 21600 Cost: tensor(8.1724, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 22000 Cost: tensor(8.1265, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 22400 Cost: tensor(8.0817, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 22800 Cost: tensor(8.0379, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 23200 Cost: tensor(7.9951, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 23600 Cost: tensor(7.9534, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 24000 Cost: tensor(7.9126, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 24400 Cost: tensor(7.8728, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 24800 Cost: tensor(7.8339, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 25200 Cost: tensor(7.7959, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 25600 Cost: tensor(7.7587, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 26000 Cost: tensor(7.7225, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 26400 Cost: tensor(7.6870, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 26800 Cost: tensor(7.6523, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 27200 Cost: tensor(7.6185, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 27600 Cost: tensor(7.5855, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 28000 Cost: tensor(7.5531, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 28400 Cost: tensor(7.5215, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 28800 Cost: tensor(7.4907, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 29200 Cost: tensor(7.4605, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 29600 Cost: tensor(7.4311, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 30000 Cost: tensor(7.4023, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 30400 Cost: tensor(7.3741, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 30800 Cost: tensor(7.3466, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 31200 Cost: tensor(7.3197, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 31600 Cost: tensor(7.2934, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 32000 Cost: tensor(7.2677, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 32400 Cost: tensor(7.2426, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 32800 Cost: tensor(7.2180, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 33200 Cost: tensor(7.1939, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 33600 Cost: tensor(7.1705, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 34000 Cost: tensor(7.1475, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 34400 Cost: tensor(7.1250, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 34800 Cost: tensor(7.1031, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 35200 Cost: tensor(7.0816, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 35600 Cost: tensor(7.0607, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 36000 Cost: tensor(7.0402, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 36400 Cost: tensor(7.0201, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 36800 Cost: tensor(7.0005, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 37200 Cost: tensor(6.9813, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 37600 Cost: tensor(6.9625, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 38000 Cost: tensor(6.9441, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 38400 Cost: tensor(6.9262, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 38800 Cost: tensor(6.9086, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 39200 Cost: tensor(6.8914, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 39600 Cost: tensor(6.8746, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 40000 Cost: tensor(6.8581, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 40400 Cost: tensor(6.8421, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 40800 Cost: tensor(6.8263, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 41200 Cost: tensor(6.8109, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 41600 Cost: tensor(6.7959, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 42000 Cost: tensor(6.7812, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 42400 Cost: tensor(6.7668, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 42800 Cost: tensor(6.7527, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 43200 Cost: tensor(6.7389, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 43600 Cost: tensor(6.7254, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 44000 Cost: tensor(6.7122, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 44400 Cost: tensor(6.6993, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 44800 Cost: tensor(6.6867, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 45200 Cost: tensor(6.6743, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 45600 Cost: tensor(6.6622, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 46000 Cost: tensor(6.6504, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 46400 Cost: tensor(6.6387, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 46800 Cost: tensor(6.6275, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 47200 Cost: tensor(6.6163, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 47600 Cost: tensor(6.6054, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 48000 Cost: tensor(6.5949, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 48400 Cost: tensor(6.5844, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 48800 Cost: tensor(6.5742, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 49200 Cost: tensor(6.5643, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 49600 Cost: tensor(6.5545, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 50000 Cost: tensor(6.5450, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 50400 Cost: tensor(6.5357, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 50800 Cost: tensor(6.5265, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 51200 Cost: tensor(6.5175, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 51600 Cost: tensor(6.5088, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 52000 Cost: tensor(6.5003, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 52400 Cost: tensor(6.4918, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 52800 Cost: tensor(6.4836, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 53200 Cost: tensor(6.4756, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 53600 Cost: tensor(6.4677, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 54000 Cost: tensor(6.4600, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 54400 Cost: tensor(6.4524, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 54800 Cost: tensor(6.4451, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 55200 Cost: tensor(6.4378, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 55600 Cost: tensor(6.4307, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 56000 Cost: tensor(6.4238, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 56400 Cost: tensor(6.4170, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 56800 Cost: tensor(6.4104, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 57200 Cost: tensor(6.4039, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 57600 Cost: tensor(6.3975, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 58000 Cost: tensor(6.3913, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 58400 Cost: tensor(6.3852, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 58800 Cost: tensor(6.3791, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 59200 Cost: tensor(6.3733, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 59600 Cost: tensor(6.3676, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 60000 Cost: tensor(6.3620, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 60400 Cost: tensor(6.3565, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 60800 Cost: tensor(6.3510, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 61200 Cost: tensor(6.3458, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 61600 Cost: tensor(6.3406, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 62000 Cost: tensor(6.3356, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 62400 Cost: tensor(6.3306, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 62800 Cost: tensor(6.3257, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 63200 Cost: tensor(6.3210, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 63600 Cost: tensor(6.3163, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 64000 Cost: tensor(6.3118, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 64400 Cost: tensor(6.3073, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 64800 Cost: tensor(6.3029, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 65200 Cost: tensor(6.2986, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 65600 Cost: tensor(6.2944, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 66000 Cost: tensor(6.2904, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 66400 Cost: tensor(6.2863, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 66800 Cost: tensor(6.2824, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 67200 Cost: tensor(6.2785, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 67600 Cost: tensor(6.2747, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 68000 Cost: tensor(6.2710, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 68400 Cost: tensor(6.2674, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 68800 Cost: tensor(6.2639, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 69200 Cost: tensor(6.2604, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 69600 Cost: tensor(6.2569, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 70000 Cost: tensor(6.2536, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 70400 Cost: tensor(6.2503, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 70800 Cost: tensor(6.2472, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 71200 Cost: tensor(6.2440, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 71600 Cost: tensor(6.2409, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 72000 Cost: tensor(6.2379, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 72400 Cost: tensor(6.2349, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 72800 Cost: tensor(6.2320, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 73200 Cost: tensor(6.2292, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 73600 Cost: tensor(6.2265, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 74000 Cost: tensor(6.2238, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 74400 Cost: tensor(6.2211, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 74800 Cost: tensor(6.2185, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 75200 Cost: tensor(6.2159, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 75600 Cost: tensor(6.2134, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 76000 Cost: tensor(6.2109, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 76400 Cost: tensor(6.2085, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 76800 Cost: tensor(6.2062, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 77200 Cost: tensor(6.2039, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 77600 Cost: tensor(6.2016, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 78000 Cost: tensor(6.1994, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 78400 Cost: tensor(6.1972, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 78800 Cost: tensor(6.1951, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 79200 Cost: tensor(6.1930, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 79600 Cost: tensor(6.1910, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 80000 Cost: tensor(6.1890, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 80400 Cost: tensor(6.1870, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 80800 Cost: tensor(6.1851, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 81200 Cost: tensor(6.1832, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 81600 Cost: tensor(6.1814, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 82000 Cost: tensor(6.1795, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 82400 Cost: tensor(6.1778, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 82800 Cost: tensor(6.1760, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 83200 Cost: tensor(6.1743, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 83600 Cost: tensor(6.1727, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 84000 Cost: tensor(6.1711, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 84400 Cost: tensor(6.1694, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 84800 Cost: tensor(6.1679, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 85200 Cost: tensor(6.1664, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 85600 Cost: tensor(6.1648, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 86000 Cost: tensor(6.1634, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 86400 Cost: tensor(6.1619, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 86800 Cost: tensor(6.1605, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 87200 Cost: tensor(6.1591, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 87600 Cost: tensor(6.1577, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 88000 Cost: tensor(6.1564, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 88400 Cost: tensor(6.1551, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 88800 Cost: tensor(6.1538, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 89200 Cost: tensor(6.1526, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 89600 Cost: tensor(6.1513, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 90000 Cost: tensor(6.1501, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 90400 Cost: tensor(6.1489, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 90800 Cost: tensor(6.1478, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 91200 Cost: tensor(6.1466, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 91600 Cost: tensor(6.1455, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 92000 Cost: tensor(6.1444, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 92400 Cost: tensor(6.1433, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 92800 Cost: tensor(6.1423, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 93200 Cost: tensor(6.1413, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 93600 Cost: tensor(6.1403, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 94000 Cost: tensor(6.1393, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 94400 Cost: tensor(6.1384, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 94800 Cost: tensor(6.1374, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 95200 Cost: tensor(6.1365, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 95600 Cost: tensor(6.1356, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 96000 Cost: tensor(6.1347, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 96400 Cost: tensor(6.1338, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 96800 Cost: tensor(6.1329, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 97200 Cost: tensor(6.1321, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 97600 Cost: tensor(6.1312, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 98000 Cost: tensor(6.1304, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 98400 Cost: tensor(6.1296, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 98800 Cost: tensor(6.1289, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 99200 Cost: tensor(6.1281, grad_fn=<MseLossBackward0>)\n",
      "Epoch: 99600 Cost: tensor(6.1274, grad_fn=<MseLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(nb_epochs):\n",
    "    prediction = model(x_train)\n",
    "    cost = F.mse_loss(prediction, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 4000 ==0:\n",
    "        print('Epoch:', epoch, 'Cost:', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
