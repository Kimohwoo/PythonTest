{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.0900, 0.2447, 0.6652])\n",
      "tensor(1.)\n"
     ]
    }
   ],
   "source": [
    "z = torch.FloatTensor([1,2,3])\n",
    "y_hat=F.softmax(z, dim=0)\n",
    "print(y_hat)\n",
    "print(y_hat.sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0.1348, 0.2964, 0.2279, 0.1248, 0.2161],\n",
      "        [0.1736, 0.2437, 0.1983, 0.1461, 0.2383],\n",
      "        [0.1966, 0.1310, 0.2301, 0.2732, 0.1692]], grad_fn=<SoftmaxBackward0>)\n",
      "tensor([1.0000, 1.0000, 1.0000], grad_fn=<SumBackward1>)\n"
     ]
    }
   ],
   "source": [
    "z = torch.rand(3,5, requires_grad=True)\n",
    "y_hat = F.softmax(z, dim=1)\n",
    "print(y_hat)\n",
    "print(y_hat.sum(dim=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 1])\n",
      "tensor([[0., 1., 0., 0., 0.],\n",
      "        [0., 0., 1., 0., 0.],\n",
      "        [0., 1., 0., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y = torch.randint(5, (3,)).long()\n",
    "print(y)\n",
    "y_one_hot = torch.zeros_like(y_hat)\n",
    "y_one_hot.scatter_(1, y.unsqueeze(1), 1)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[1],\n",
      "        [2],\n",
      "        [1]])\n"
     ]
    }
   ],
   "source": [
    "print(y.unsqueeze(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x146a112aa10>"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([8, 4])\n",
      "torch.Size([8])\n"
     ]
    }
   ],
   "source": [
    "x_data = [[1,2,1,1],\n",
    "          [2,1,3,2],\n",
    "          [3,1,3,4],\n",
    "          [4,1,5,5],\n",
    "          [1,7,5,5],\n",
    "          [1,2,5,6],\n",
    "          [1,6,6,6],\n",
    "          [1,7,7,7]]\n",
    "y_data = [2,2,2,1,1,1,0,0]\n",
    "x_train = torch.FloatTensor(x_data)\n",
    "y_train = torch.LongTensor(y_data)\n",
    "print(x_train.shape)\n",
    "print(y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 0., 1.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [0., 1., 0.],\n",
      "        [1., 0., 0.],\n",
      "        [1., 0., 0.]])\n"
     ]
    }
   ],
   "source": [
    "y_one_hot = torch.zeros(8,3)\n",
    "y_one_hot.scatter_(1,y_train.unsqueeze(1),1)\n",
    "print(y_one_hot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "W = torch.zeros((4,3),requires_grad=True)\n",
    "b = torch.zeros((1,3),requires_grad=True)\n",
    "optimize = optim.SGD([W,b], lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: tensor(1.0986, grad_fn=<MeanBackward0>)\n",
      "Epoch: 100 Cost: tensor(0.7042, grad_fn=<MeanBackward0>)\n",
      "Epoch: 200 Cost: tensor(0.6230, grad_fn=<MeanBackward0>)\n",
      "Epoch: 300 Cost: tensor(0.5657, grad_fn=<MeanBackward0>)\n",
      "Epoch: 400 Cost: tensor(0.5153, grad_fn=<MeanBackward0>)\n",
      "Epoch: 500 Cost: tensor(0.4677, grad_fn=<MeanBackward0>)\n",
      "Epoch: 600 Cost: tensor(0.4213, grad_fn=<MeanBackward0>)\n",
      "Epoch: 700 Cost: tensor(0.3754, grad_fn=<MeanBackward0>)\n",
      "Epoch: 800 Cost: tensor(0.3298, grad_fn=<MeanBackward0>)\n",
      "Epoch: 900 Cost: tensor(0.2851, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1000 Cost: tensor(0.2482, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1100 Cost: tensor(0.2327, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1200 Cost: tensor(0.2214, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1300 Cost: tensor(0.2111, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1400 Cost: tensor(0.2017, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1500 Cost: tensor(0.1931, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1600 Cost: tensor(0.1852, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1700 Cost: tensor(0.1778, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1800 Cost: tensor(0.1710, grad_fn=<MeanBackward0>)\n",
      "Epoch: 1900 Cost: tensor(0.1647, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2000 Cost: tensor(0.1588, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2100 Cost: tensor(0.1533, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2200 Cost: tensor(0.1482, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2300 Cost: tensor(0.1434, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2400 Cost: tensor(0.1388, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2500 Cost: tensor(0.1346, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2600 Cost: tensor(0.1306, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2700 Cost: tensor(0.1268, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2800 Cost: tensor(0.1232, grad_fn=<MeanBackward0>)\n",
      "Epoch: 2900 Cost: tensor(0.1198, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3000 Cost: tensor(0.1166, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3100 Cost: tensor(0.1136, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3200 Cost: tensor(0.1107, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3300 Cost: tensor(0.1079, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3400 Cost: tensor(0.1053, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3500 Cost: tensor(0.1028, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3600 Cost: tensor(0.1004, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3700 Cost: tensor(0.0981, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3800 Cost: tensor(0.0959, grad_fn=<MeanBackward0>)\n",
      "Epoch: 3900 Cost: tensor(0.0938, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4000 Cost: tensor(0.0918, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4100 Cost: tensor(0.0899, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4200 Cost: tensor(0.0880, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4300 Cost: tensor(0.0863, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4400 Cost: tensor(0.0845, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4500 Cost: tensor(0.0829, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4600 Cost: tensor(0.0813, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4700 Cost: tensor(0.0798, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4800 Cost: tensor(0.0783, grad_fn=<MeanBackward0>)\n",
      "Epoch: 4900 Cost: tensor(0.0769, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5000 Cost: tensor(0.0756, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5100 Cost: tensor(0.0742, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5200 Cost: tensor(0.0730, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5300 Cost: tensor(0.0717, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5400 Cost: tensor(0.0705, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5500 Cost: tensor(0.0694, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5600 Cost: tensor(0.0683, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5700 Cost: tensor(0.0672, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5800 Cost: tensor(0.0661, grad_fn=<MeanBackward0>)\n",
      "Epoch: 5900 Cost: tensor(0.0651, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6000 Cost: tensor(0.0641, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6100 Cost: tensor(0.0632, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6200 Cost: tensor(0.0622, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6300 Cost: tensor(0.0613, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6400 Cost: tensor(0.0605, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6500 Cost: tensor(0.0596, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6600 Cost: tensor(0.0588, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6700 Cost: tensor(0.0580, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6800 Cost: tensor(0.0572, grad_fn=<MeanBackward0>)\n",
      "Epoch: 6900 Cost: tensor(0.0564, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7000 Cost: tensor(0.0557, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7100 Cost: tensor(0.0549, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7200 Cost: tensor(0.0542, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7300 Cost: tensor(0.0535, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7400 Cost: tensor(0.0529, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7500 Cost: tensor(0.0522, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7600 Cost: tensor(0.0516, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7700 Cost: tensor(0.0509, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7800 Cost: tensor(0.0503, grad_fn=<MeanBackward0>)\n",
      "Epoch: 7900 Cost: tensor(0.0497, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8000 Cost: tensor(0.0491, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8100 Cost: tensor(0.0486, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8200 Cost: tensor(0.0480, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8300 Cost: tensor(0.0475, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8400 Cost: tensor(0.0469, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8500 Cost: tensor(0.0464, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8600 Cost: tensor(0.0459, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8700 Cost: tensor(0.0454, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8800 Cost: tensor(0.0449, grad_fn=<MeanBackward0>)\n",
      "Epoch: 8900 Cost: tensor(0.0444, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9000 Cost: tensor(0.0440, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9100 Cost: tensor(0.0435, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9200 Cost: tensor(0.0431, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9300 Cost: tensor(0.0426, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9400 Cost: tensor(0.0422, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9500 Cost: tensor(0.0418, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9600 Cost: tensor(0.0414, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9700 Cost: tensor(0.0410, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9800 Cost: tensor(0.0406, grad_fn=<MeanBackward0>)\n",
      "Epoch: 9900 Cost: tensor(0.0402, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 10000\n",
    "for epoch in range(n_epochs):\n",
    "    y_hat = F.softmax(x_train.matmul(W) + b, dim=1)\n",
    "    cost = (y_one_hot * -torch.log(y_hat)).sum(dim=1).mean()\n",
    "\n",
    "    optimize.zero_grad()\n",
    "    cost.backward()\n",
    "    optimize.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', epoch, 'Cost:', cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = nn.Linear(4,3)\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "n_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: tensor(1.6168, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 100 Cost: tensor(0.6589, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 200 Cost: tensor(0.5734, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 300 Cost: tensor(0.5182, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 400 Cost: tensor(0.4733, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 500 Cost: tensor(0.4335, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 600 Cost: tensor(0.3966, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 700 Cost: tensor(0.3609, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 800 Cost: tensor(0.3254, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 900 Cost: tensor(0.2892, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1000 Cost: tensor(0.2541, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1100 Cost: tensor(0.2350, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1200 Cost: tensor(0.2235, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1300 Cost: tensor(0.2131, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1400 Cost: tensor(0.2035, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1500 Cost: tensor(0.1948, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1600 Cost: tensor(0.1867, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1700 Cost: tensor(0.1792, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1800 Cost: tensor(0.1723, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1900 Cost: tensor(0.1659, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2000 Cost: tensor(0.1600, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2100 Cost: tensor(0.1544, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2200 Cost: tensor(0.1492, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2300 Cost: tensor(0.1443, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2400 Cost: tensor(0.1397, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2500 Cost: tensor(0.1354, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2600 Cost: tensor(0.1314, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2700 Cost: tensor(0.1275, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2800 Cost: tensor(0.1239, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2900 Cost: tensor(0.1205, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3000 Cost: tensor(0.1172, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3100 Cost: tensor(0.1142, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3200 Cost: tensor(0.1112, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3300 Cost: tensor(0.1084, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3400 Cost: tensor(0.1058, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3500 Cost: tensor(0.1033, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3600 Cost: tensor(0.1008, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3700 Cost: tensor(0.0985, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3800 Cost: tensor(0.0963, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3900 Cost: tensor(0.0942, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4000 Cost: tensor(0.0922, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4100 Cost: tensor(0.0903, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4200 Cost: tensor(0.0884, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4300 Cost: tensor(0.0866, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4400 Cost: tensor(0.0849, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4500 Cost: tensor(0.0832, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4600 Cost: tensor(0.0816, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4700 Cost: tensor(0.0801, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4800 Cost: tensor(0.0786, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4900 Cost: tensor(0.0772, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5000 Cost: tensor(0.0758, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5100 Cost: tensor(0.0745, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5200 Cost: tensor(0.0732, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5300 Cost: tensor(0.0720, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5400 Cost: tensor(0.0708, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5500 Cost: tensor(0.0696, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5600 Cost: tensor(0.0685, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5700 Cost: tensor(0.0674, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5800 Cost: tensor(0.0663, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5900 Cost: tensor(0.0653, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6000 Cost: tensor(0.0643, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6100 Cost: tensor(0.0634, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6200 Cost: tensor(0.0624, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6300 Cost: tensor(0.0615, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6400 Cost: tensor(0.0606, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6500 Cost: tensor(0.0598, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6600 Cost: tensor(0.0589, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6700 Cost: tensor(0.0581, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6800 Cost: tensor(0.0573, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6900 Cost: tensor(0.0566, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7000 Cost: tensor(0.0558, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7100 Cost: tensor(0.0551, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7200 Cost: tensor(0.0544, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7300 Cost: tensor(0.0537, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7400 Cost: tensor(0.0530, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7500 Cost: tensor(0.0523, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7600 Cost: tensor(0.0517, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7700 Cost: tensor(0.0511, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7800 Cost: tensor(0.0504, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7900 Cost: tensor(0.0498, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8000 Cost: tensor(0.0493, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8100 Cost: tensor(0.0487, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8200 Cost: tensor(0.0481, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8300 Cost: tensor(0.0476, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8400 Cost: tensor(0.0470, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8500 Cost: tensor(0.0465, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8600 Cost: tensor(0.0460, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8700 Cost: tensor(0.0455, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8800 Cost: tensor(0.0450, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8900 Cost: tensor(0.0445, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9000 Cost: tensor(0.0441, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9100 Cost: tensor(0.0436, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9200 Cost: tensor(0.0432, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9300 Cost: tensor(0.0427, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9400 Cost: tensor(0.0423, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9500 Cost: tensor(0.0419, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9600 Cost: tensor(0.0415, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9700 Cost: tensor(0.0410, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9800 Cost: tensor(0.0406, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9900 Cost: tensor(0.0403, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    y_hat = model(x_train)\n",
    "    cost = F.cross_entropy(y_hat, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 ==0:\n",
    "        print('Epoch:', epoch, 'Cost:', cost.item())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-11.4186,  -0.1089,  11.8237],\n",
      "        [ -4.3004,   0.4548,   4.3716],\n",
      "        [-18.3864,   8.3331,  11.4630],\n",
      "        [-12.6069,   8.7536,   5.5109],\n",
      "        [  0.9640,   3.5058,  -2.2738],\n",
      "        [  4.2153,   7.4299,  -9.0252],\n",
      "        [  7.0719,   4.7319,  -9.1731],\n",
      "        [ 10.5022,   5.6714, -13.0725]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prodiction = model(x_train)\n",
    "print(prodiction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SoftmaxClassModel(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.linear = nn.Linear(4,3)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SoftmaxClassModel()\n",
    "optimizer = optim.SGD(model.parameters(), lr=0.1)\n",
    "n_epochs = 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0 Cost: tensor(2.6376, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 100 Cost: tensor(0.6479, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 200 Cost: tensor(0.5646, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 300 Cost: tensor(0.5110, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 400 Cost: tensor(0.4672, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 500 Cost: tensor(0.4283, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 600 Cost: tensor(0.3919, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 700 Cost: tensor(0.3567, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 800 Cost: tensor(0.3216, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 900 Cost: tensor(0.2856, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1000 Cost: tensor(0.2508, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1100 Cost: tensor(0.2321, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1200 Cost: tensor(0.2209, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1300 Cost: tensor(0.2106, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1400 Cost: tensor(0.2013, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1500 Cost: tensor(0.1927, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1600 Cost: tensor(0.1848, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1700 Cost: tensor(0.1774, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1800 Cost: tensor(0.1707, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 1900 Cost: tensor(0.1644, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2000 Cost: tensor(0.1585, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2100 Cost: tensor(0.1530, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2200 Cost: tensor(0.1479, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2300 Cost: tensor(0.1431, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2400 Cost: tensor(0.1386, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2500 Cost: tensor(0.1343, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2600 Cost: tensor(0.1303, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2700 Cost: tensor(0.1266, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2800 Cost: tensor(0.1230, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 2900 Cost: tensor(0.1196, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3000 Cost: tensor(0.1164, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3100 Cost: tensor(0.1134, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3200 Cost: tensor(0.1105, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3300 Cost: tensor(0.1077, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3400 Cost: tensor(0.1051, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3500 Cost: tensor(0.1026, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3600 Cost: tensor(0.1002, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3700 Cost: tensor(0.0979, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3800 Cost: tensor(0.0957, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 3900 Cost: tensor(0.0937, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4000 Cost: tensor(0.0916, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4100 Cost: tensor(0.0897, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4200 Cost: tensor(0.0879, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4300 Cost: tensor(0.0861, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4400 Cost: tensor(0.0844, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4500 Cost: tensor(0.0828, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4600 Cost: tensor(0.0812, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4700 Cost: tensor(0.0797, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4800 Cost: tensor(0.0782, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 4900 Cost: tensor(0.0768, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5000 Cost: tensor(0.0754, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5100 Cost: tensor(0.0741, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5200 Cost: tensor(0.0729, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5300 Cost: tensor(0.0716, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5400 Cost: tensor(0.0704, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5500 Cost: tensor(0.0693, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5600 Cost: tensor(0.0682, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5700 Cost: tensor(0.0671, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5800 Cost: tensor(0.0660, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 5900 Cost: tensor(0.0650, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6000 Cost: tensor(0.0640, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6100 Cost: tensor(0.0631, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6200 Cost: tensor(0.0622, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6300 Cost: tensor(0.0612, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6400 Cost: tensor(0.0604, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6500 Cost: tensor(0.0595, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6600 Cost: tensor(0.0587, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6700 Cost: tensor(0.0579, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6800 Cost: tensor(0.0571, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 6900 Cost: tensor(0.0563, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7000 Cost: tensor(0.0556, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7100 Cost: tensor(0.0549, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7200 Cost: tensor(0.0542, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7300 Cost: tensor(0.0535, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7400 Cost: tensor(0.0528, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7500 Cost: tensor(0.0521, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7600 Cost: tensor(0.0515, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7700 Cost: tensor(0.0509, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7800 Cost: tensor(0.0503, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 7900 Cost: tensor(0.0497, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8000 Cost: tensor(0.0491, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8100 Cost: tensor(0.0485, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8200 Cost: tensor(0.0480, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8300 Cost: tensor(0.0474, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8400 Cost: tensor(0.0469, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8500 Cost: tensor(0.0464, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8600 Cost: tensor(0.0459, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8700 Cost: tensor(0.0454, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8800 Cost: tensor(0.0449, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 8900 Cost: tensor(0.0444, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9000 Cost: tensor(0.0439, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9100 Cost: tensor(0.0435, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9200 Cost: tensor(0.0430, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9300 Cost: tensor(0.0426, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9400 Cost: tensor(0.0422, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9500 Cost: tensor(0.0417, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9600 Cost: tensor(0.0413, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9700 Cost: tensor(0.0409, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9800 Cost: tensor(0.0405, grad_fn=<NllLossBackward0>)\n",
      "Epoch: 9900 Cost: tensor(0.0401, grad_fn=<NllLossBackward0>)\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(n_epochs):\n",
    "    y_hat = model(x_train)\n",
    "    cost = F.cross_entropy(y_hat, y_train)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    cost.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    if epoch % 100 == 0:\n",
    "        print('Epoch:', epoch, 'Cost:', cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-11.5036,  -0.2332,  11.5479],\n",
      "        [ -4.5121,   0.3314,   4.2517],\n",
      "        [-18.6580,   8.3967,  11.5297],\n",
      "        [-13.0537,   8.7771,   5.5309],\n",
      "        [  0.2068,   2.7514,  -3.0321],\n",
      "        [  3.6674,   6.8846,  -8.6047],\n",
      "        [  6.2557,   3.9119,  -9.5963],\n",
      "        [  9.5301,   4.7040, -13.5656]], grad_fn=<AddmmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "prediction = model(x_train)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
